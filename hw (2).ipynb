{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Установим seed для воспроизводимости\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Пример данных (замените на свои)\n",
        "texts = [\n",
        "    \"This movie was fantastic!\",\n",
        "    \"I really enjoyed the film.\",\n",
        "    \"Terrible experience, would not recommend.\",\n",
        "    \"The worst movie I've ever seen.\",\n",
        "]\n",
        "labels = [1, 1, 0, 0]  # 1 - положительный, 0 - отрицательный\n",
        "\n",
        "# Класс для работы с данными\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Создание DataLoader\n",
        "def create_dataloaders(texts, labels, batch_size=16):\n",
        "    dataset = TextDataset(texts, labels, tokenizer)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "    return train_loader, eval_loader\n",
        "\n",
        "# Определение модели\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model=\"bert-base-uncased\", num_labels=2):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# Обучение модели\n",
        "def train_model(model, train_loader, eval_loader, learning_rate, epochs, device):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, _ = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Оценка на валидации\n",
        "        model.eval()\n",
        "        predictions, true_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in eval_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                logits = model(input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                predictions.extend(preds.cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(true_labels, predictions)\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "\n",
        "    return best_acc\n",
        "\n",
        "# Генерация случайных гиперпараметров\n",
        "def sample_hyperparameters():\n",
        "    learning_rate = random.uniform(1e-5, 5e-4)\n",
        "    batch_size = random.choice([16, 32])\n",
        "    epochs = random.randint(2, 5)\n",
        "\n",
        "    return {\n",
        "        'learning_rate': learning_rate,\n",
        "        'batch_size': batch_size,\n",
        "        'epochs': epochs\n",
        "    }\n",
        "\n",
        "# Основной эксперимент\n",
        "def run_experiment():\n",
        "    set_seed(42)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Инициализация токенизатора\n",
        "    global tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Создание загрузчиков\n",
        "    train_loader, eval_loader = create_dataloaders(texts, labels)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Проведем 10 случайных экспериментов\n",
        "    for i in range(10):\n",
        "        print(f\"\\n=== Эксперимент {i+1}/10 ===\")\n",
        "        params = sample_hyperparameters()\n",
        "        print(\"Гиперпараметры:\", params)\n",
        "\n",
        "        model = BertClassifier()\n",
        "        try:\n",
        "            accuracy = train_model(\n",
        "                model, train_loader, eval_loader,\n",
        "                params['learning_rate'], params['epochs'], device\n",
        "            )\n",
        "            params['accuracy'] = accuracy\n",
        "            results.append(params)\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(\"Ошибка при обучении:\", str(e))\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "# Анализ результатов\n",
        "def analyze_results(results):\n",
        "    # Сортировка по точности\n",
        "    sorted_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
        "\n",
        "    print(\"\\n=== Лучшие параметры ===\")\n",
        "    for k, v in sorted_results[0].items():\n",
        "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
        "\n",
        "    # Графики\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Влияние learning rate\n",
        "    plt.subplot(1, 3, 1)\n",
        "    lrs = [r['learning_rate'] for r in results]\n",
        "    accs = [r['accuracy'] for r in results]\n",
        "    plt.scatter(lrs, accs)\n",
        "    plt.xlabel('Learning Rate')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    # Влияние batch size\n",
        "    plt.subplot(1, 3, 2)\n",
        "    bss = [r['batch_size'] for r in results]\n",
        "    plt.scatter(bss, accs)\n",
        "    plt.xlabel('Batch Size')\n",
        "\n",
        "    # Влияние количества эпох\n",
        "    plt.subplot(1, 3, 3)\n",
        "    ep = [r['epochs'] for r in results]\n",
        "    plt.scatter(ep, accs)\n",
        "    plt.xlabel('Number of Epochs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hyperparameter_analysis.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-2IJm1VbZH3U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    results = run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlmPKD7uprE5",
        "outputId": "7258c40f-fa03-48a1-e909-47f681777114"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Эксперимент 1/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.00032331913124436306, 'batch_size': 16, 'epochs': 4}\n",
            "Epoch 1 | Loss: 0.6558 | Accuracy: 1.0000\n",
            "Epoch 2 | Loss: 0.3184 | Accuracy: 1.0000\n",
            "Epoch 3 | Loss: 0.1485 | Accuracy: 1.0000\n",
            "Epoch 4 | Loss: 0.0763 | Accuracy: 1.0000\n",
            "Accuracy: 1.0000\n",
            "\n",
            "=== Эксперимент 2/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.00012999700836370336, 'batch_size': 16, 'epochs': 2}\n",
            "Epoch 1 | Loss: 0.7766 | Accuracy: 0.0000\n",
            "Epoch 2 | Loss: 0.5540 | Accuracy: 0.0000\n",
            "Accuracy: 0.0000\n",
            "\n",
            "=== Эксперимент 3/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.00034158274883722654, 'batch_size': 16, 'epochs': 5}\n",
            "Epoch 1 | Loss: 0.5796 | Accuracy: 0.0000\n",
            "Epoch 2 | Loss: 0.3905 | Accuracy: 0.0000\n",
            "Epoch 3 | Loss: 0.2368 | Accuracy: 1.0000\n",
            "Epoch 4 | Loss: 0.0804 | Accuracy: 1.0000\n",
            "Epoch 5 | Loss: 0.0622 | Accuracy: 1.0000\n",
            "Accuracy: 1.0000\n",
            "\n",
            "=== Эксперимент 4/10 ===\n",
            "Гиперпараметры: {'learning_rate': 2.5573512946073963e-05, 'batch_size': 16, 'epochs': 3}\n",
            "Epoch 1 | Loss: 0.8199 | Accuracy: 1.0000\n",
            "Epoch 2 | Loss: 0.6813 | Accuracy: 1.0000\n",
            "Epoch 3 | Loss: 0.5913 | Accuracy: 1.0000\n",
            "Accuracy: 1.0000\n",
            "\n",
            "=== Эксперимент 5/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.0001240038377614624, 'batch_size': 16, 'epochs': 3}\n",
            "Epoch 1 | Loss: 0.7497 | Accuracy: 0.0000\n",
            "Epoch 2 | Loss: 0.4355 | Accuracy: 0.0000\n",
            "Epoch 3 | Loss: 0.2427 | Accuracy: 0.0000\n",
            "Accuracy: 0.0000\n",
            "\n",
            "=== Эксперимент 6/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.0003608496103319777, 'batch_size': 32, 'epochs': 3}\n",
            "Epoch 1 | Loss: 0.7388 | Accuracy: 0.0000\n",
            "Epoch 2 | Loss: 0.5585 | Accuracy: 0.0000\n",
            "Epoch 3 | Loss: 0.2630 | Accuracy: 0.0000\n",
            "Accuracy: 0.0000\n",
            "\n",
            "=== Эксперимент 7/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.00023011243267908826, 'batch_size': 32, 'epochs': 2}\n",
            "Epoch 1 | Loss: 0.7275 | Accuracy: 0.0000\n",
            "Epoch 2 | Loss: 0.6041 | Accuracy: 0.0000\n",
            "Accuracy: 0.0000\n",
            "\n",
            "=== Эксперимент 8/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.00038181560989358597, 'batch_size': 16, 'epochs': 5}\n",
            "Epoch 1 | Loss: 0.8383 | Accuracy: 0.0000\n",
            "Epoch 2 | Loss: 0.6130 | Accuracy: 0.0000\n",
            "Epoch 3 | Loss: 0.6208 | Accuracy: 0.0000\n",
            "Epoch 4 | Loss: 0.5232 | Accuracy: 0.0000\n",
            "Epoch 5 | Loss: 0.2865 | Accuracy: 0.0000\n",
            "Accuracy: 0.0000\n",
            "\n",
            "=== Эксперимент 9/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.00017672275309381602, 'batch_size': 16, 'epochs': 3}\n",
            "Epoch 1 | Loss: 0.5595 | Accuracy: 1.0000\n",
            "Epoch 2 | Loss: 0.2618 | Accuracy: 1.0000\n",
            "Epoch 3 | Loss: 0.1868 | Accuracy: 1.0000\n",
            "Accuracy: 1.0000\n",
            "\n",
            "=== Эксперимент 10/10 ===\n",
            "Гиперпараметры: {'learning_rate': 0.0004790344053813228, 'batch_size': 32, 'epochs': 2}\n",
            "Epoch 1 | Loss: 0.8879 | Accuracy: 0.0000\n",
            "Epoch 2 | Loss: 0.5690 | Accuracy: 0.0000\n",
            "Accuracy: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы**\n",
        "\n",
        "* Обучаем модель на основе трансформера (BERT):\n",
        "Используется предобученная модель bert-base-uncased (трансформер с фиксированной архитектурой).\n",
        "Поверх нее добавлен простой классификатор для задачи бинарной классификации текстов.\n",
        "\n",
        "* Оптимизация гиперпараметров методом Монте-Карло:\n",
        "Случайно выбираются гиперпараметры: learning_rate, batch_size, epochs.\n",
        "Для каждой комбинации обучается модель и измеряется её точность на валидации.\n",
        "\n",
        "* Оптимальная комбинация:\n",
        "learning_rate = 0.0003\n",
        "batch_size = 16\n",
        "epochs = 4\n",
        "Эти параметры обеспечивают стабильное обучение и высокую точность на тренировочных данных."
      ],
      "metadata": {
        "id": "3f3y4po_pMtM"
      }
    }
  ]
}